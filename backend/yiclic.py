# -*- coding: utf-8 -*-
"""YICLIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iHsoKEYtpDmo_QrECtSRXhLUy-aJANlS

# YICLIC

## import and install packages
"""

# !pip install google-search-results openai
# !pip install chardet
# !pip install transformers torch jieba scipy pandas numpy tqdm
# !pip install fastparquet

# # å‡ç´š safetensors åº«
# !pip install --upgrade safetensors

# # å»ºè­°åŒæ™‚å‡ç´š transformers åº«ï¼Œä»¥ç¢ºä¿ç›¸å®¹æ€§
# !pip install --upgrade transformers

from pathlib import Path
import csv
import re
import os
import chardet
import time
import math
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

from serpapi import GoogleSearch
from openai import OpenAI

# ml
import importlib, lexicon_utils
importlib.reload(lexicon_utils)
from inference_food import predict_food_on_csv

BASE_DIR = Path(__file__).resolve().parent
OUTPUTS_DIR = BASE_DIR / "outputs"
REVIEWS_DIR = BASE_DIR / "reviews"

"""## API key è¨­å®šæ–¹æ³•
### Step 1 â€” å°‡ Google Maps(SERPAPI) èˆ‡ OpenAI API Key å¯«å…¥ä½¿ç”¨è€…å®¶ç›®éŒ„ä¸‹çš„ .zshrc è¨­å®šæª”

### Step 2 â€” åœ¨ .zshrc ä¸­åŠ å…¥ä»¥ä¸‹ç’°å¢ƒè®Šæ•¸è¨­å®š
> æ³¨æ„ï¼š= å·¦å³ä¸èƒ½æœ‰ç©ºæ ¼

```
export SERPAPI_API_KEY="ä½ çš„ Google Maps Key"
export GPT_API_KEY="ä½ çš„ OpenAI API Key"
```

### Step 3 â€” å„²å­˜å¾Œé‡æ–°è¼‰å…¥è¨­å®šæª”
```
source ~/.zshrc
```

### å¯é¸ï¼šç¢ºèªå·²æˆåŠŸè¼‰å…¥è®Šæ•¸

```
echo $SERPAPI_API_KEY
echo $GPT_API_KEY
```
"""

# API KEY SETTING
try:
    SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")
    GPT_API_KEY = os.getenv("GPT_API_KEY")
    print(repr(os.getenv("SERPAPI_API_KEY")))
    # print(repr(os.getenv("GPT_API_KEY")))

    client = OpenAI(api_key = GPT_API_KEY)
    print("API Key å·²æ­£ç¢ºè¼‰å…¥!")
except Exception as err:
    print(err)

"""## Capture the command from the Google API"""

# # æŠ“ place_id
# params = {
#     "engine": "google_maps",
#     "q": "æ—è¨˜è±†æ¼¿åº—",
#     "hl": "zh-TW",
#     "api_key": SERPAPI_API_KEY,
# }
# search = GoogleSearch(params)
# results = search.get_dict()

# print(json.dumps(results, indent=2, ensure_ascii=False))

# Google map prompt è¨­å®š
# è¨­å®šï¼šç›®æ¨™åº—å®¶ (ä½¿ç”¨ place_id)
TARGET_SHOPS = [
    {"name": "æ—è¨˜è±†æ¼¿åº—", "place_id": "ChIJZZYstakFbjQRgfM2o6Ibc78"},  # æ›¿æ›æˆçœŸå¯¦ place_id
    {"name": "å¤©ä¸€é¦™è‚‰ç„¿é †", "place_id": "ChIJKybdGT5OXTQRKjrGMZ9BeFk"},
]

# æ¯å®¶åº—æœ€å¤§æŠ“å–è©•è«–æ•¸é‡
MAX_REVIEWS_PER_SHOP = 5
# æ¯å®¶åº—æœ€å¤šæŠ“å–é æ•¸
MAX_PAGES_PER_SHOP = 3

# å·¥å…·å‡½å¼
def contains_chinese(text):
    pattern = re.compile(r'[\u4e00-\u9fff]+')
    return bool(pattern.search(text))

# æŠ“è©•è«–
def fetch_reviews(place_id, api_key, next_page_token=None):
    params = {
        "engine": "google_maps_reviews",
        "place_id": place_id,  # â† ä½ çœŸæ­£è¦ç”¨çš„ key
        "api_key": api_key,
        "hl": "zh-TW"
    }

    if next_page_token:
        params["next_page_token"] = next_page_token

    search = GoogleSearch(params)
    results = search.get_dict()
    return results, results.get("reviews", [])

# è™•ç†è©•è«–ï¼ˆé™åˆ¶æ¯åº—è©•è«–æ•¸é‡ & åˆ†é ï¼‰
def process_all_reviews_limited(api_key, target_shops, max_reviews_per_shop=50, max_pages_per_shop=3):
    all_reviews = []

    for shop in target_shops:
        place_id = shop["place_id"]
        restaurant_name = shop["name"]
        print(f"\n--- è™•ç†: {restaurant_name} (ID: {place_id}) ---")

        next_token = None
        page_count = 0
        reviews_for_shop = 0

        while True:
            results, reviews = fetch_reviews(place_id, api_key, next_token)
            # print("DEBUG results:", results)
            # print("DEBUG reviews:", reviews)
            if not reviews:
                print(f"âš ï¸ {restaurant_name} æ²’æœ‰è©•è«–")
                break

            for review in reviews:
                if reviews_for_shop >= max_reviews_per_shop:
                    print(f"é”åˆ° {restaurant_name} æœ€å¤§è©•è«–æ•¸é‡é™åˆ¶ ({max_reviews_per_shop})")
                    break

                rating = review.get("rating", "")
                date = review.get("date", "")
                snippet = review.get("snippet", "")

                if snippet != "" and contains_chinese(snippet):
                    all_reviews.append({
                        "shop": restaurant_name,
                        "snippet": snippet,
                        "date": date,
                        "rating": rating
                    })
                    reviews_for_shop += 1

            if reviews_for_shop >= max_reviews_per_shop:
                break

            next_token = results.get("serpapi_pagination", {}).get("next_page_token")
            page_count += 1
            if not next_token or page_count >= max_pages_per_shop:
                if page_count >= max_pages_per_shop:
                    print(f"é”åˆ° {restaurant_name} æœ€å¤§é æ•¸é™åˆ¶ ({max_pages_per_shop})")
                break

    return all_reviews

# CSV å„²å­˜ï¼ˆè¦†å¯«ï¼‰
def append_to_csv(data, filename):
    fieldnames = ["shop", "snippet", "date", "rating"]

    # ä½¿ç”¨ "w" è¦†å¯«æ•´å€‹æ–‡ä»¶ï¼ˆåŸæœ¬æ˜¯ "a"ï¼‰
    with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        # è¦†å¯«æ¨¡å¼ä¸€å®šè¦é‡æ–°å¯« header
        writer.writeheader()

        for row in data:
            writer.writerow({
                "shop": row.get("shop", ""),
                "snippet": row.get("snippet", ""),
                "date": row.get("date", ""),
                "rating": row.get("rating", "")
            })

def run_pipeline(
    target_shops=None,
    api_key: str | None = None,
    max_reviews_per_shop=MAX_REVIEWS_PER_SHOP,
    max_pages_per_shop=MAX_PAGES_PER_SHOP,
):
    """åŸ·è¡Œæ•´æ¢æµç¨‹ï¼Œå›å‚³æ‰€æœ‰è©•è«–èˆ‡åº—å®¶åˆ†æ•¸æ’åºçµæœã€‚"""
    target_shops = target_shops or TARGET_SHOPS
    api_key = api_key or SERPAPI_API_KEY

    REVIEWS_DIR.mkdir(exist_ok=True)
    print(f"ğŸ“‚ è³‡æ–™å¤¾ '{REVIEWS_DIR}' å·²å»ºç«‹æˆ–å·²å­˜åœ¨")

    output_filename = REVIEWS_DIR / "multi_location_reviews_for_labeling.csv"

    collected_data = process_all_reviews_limited(
        api_key,
        target_shops,
        max_reviews_per_shop=max_reviews_per_shop,
        max_pages_per_shop=max_pages_per_shop
    )

    if collected_data:
        append_to_csv(collected_data, output_filename)
        print(f"\n--- æˆåŠŸï¼å…± {len(collected_data)} æ¢è©•è«–å·²è¿½åŠ åˆ° {output_filename} ---")
    else:
        print("\nâš ï¸ æ²’æœ‰æ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆçš„è©•è«–è³‡æ–™ã€‚")

    input_csv = output_filename
    output_csv = BASE_DIR / "multi_location_reviews_with_rank_test_pred.csv"
    filtered_output_csv = BASE_DIR / "multi_location_reviews_with_rank_test_food_only.csv"
    model_dir = OUTPUTS_DIR / "bert_food_lexicon_best"

    # === Step 1: å…ˆæŠŠ CSV ä¸Ÿçµ¦ ML éæ¿¾ ===
    df_all, df_food = predict_food_on_csv(
        input_csv=str(input_csv),
        output_csv=str(output_csv),
        filtered_output_csv=str(filtered_output_csv),
        model_dir=str(model_dir),
        out_dir_lexicon=str(OUTPUTS_DIR),
        encoding="utf-8-sig",
        threshold=0.5,
    )

    print(f"ML éæ¿¾å¾Œï¼Œé£Ÿç‰©ç›¸é—œè©•è«–å…± {len(df_food)} æ¢")

    # === Step 2: å¾ df_food["snippet"] è£¡é¢å–å‡ºæ¸…ç†å¾Œçš„è©•è«– ===
    all_reviews_list = []
    for snippet in df_food["snippet"]:
        clean_snippet = str(snippet).replace("\n", " ").strip()
        all_reviews_list.append(clean_snippet)

    print(f"âœ… all_reviews_list å·²å»ºç«‹ï¼Œå…± {len(all_reviews_list)} æ¢è©•è«–")

    # === Step 3: è½‰æˆä½ åŸæœ¬ä½¿ç”¨çš„æ ¼å¼ï¼ˆç·¨è™Ÿ + joinï¼‰ ===
    all_reviews_content = "\n\n".join(
        [f"{i+1}. {r}" for i, r in enumerate(all_reviews_list)]
    )

    # æ¸¬è©¦å°å‡ºå‰ 1000 å­—
    print(all_reviews_content[:1000])

    """## GPT API - Output the manual review rank"""

    scoring_criteria = """ä½ æ˜¯ä¸€å€‹è©•è«–æ¨™è¨»å“¡
    ä»»å‹™ï¼šæ ¹æ“šè©•è«–å…§å®¹ï¼Œæ¨™è¨˜é£Ÿç‰©è©•åˆ†(1~5 åˆ†ï¼Œå¯å« 0.5)
    è¦å‰‡ï¼š
    åªè©•é£Ÿç‰©(ä¸è€ƒæ…®åƒ¹æ ¼ã€ç’°å¢ƒã€æœå‹™)
    è‹¥è©•è«–æœ‰ã€Œæ‰“å¡æ›¬è´ˆå“ã€æŠ˜æ‰£ã€é€å°èœã€ç­‰ï¼Œè©•åˆ† -0.5
    è©•åˆ†åŸºæº–ï¼š
    1ï¼šæ¥µé›£åƒ
    2ï¼šé›£åƒ
    3ï¼šæ™®é€š
    4ï¼šå¥½åƒ
    5ï¼šéå¸¸å¥½åƒ
    è¼¸å…¥æ ¼å¼ï¼šæ¯å€‹è©•è«–é–“éš”å…©å€‹æ›è¡Œï¼Œæ²’æœ‰é‡åˆ°å…©å€‹æ›è¡Œå‰ç‚ºåŒä¸€å€‹è©•è«–
    è¼¸å‡ºæ ¼å¼ï¼š
    åªè¼¸å‡ºåˆ†æ•¸(å¦‚ï¼š3.5)
    """

    # ====== çµ„ Prompt ======
    train_prompt = (
        scoring_criteria
        + "\n------------------------------------------\n"
        + "ç¾åœ¨è«‹ä¾ç…§ä»¥ä¸Šè¦å‰‡ï¼Œå¹«æˆ‘ç‚ºåº•ä¸‹çš„é¤å»³è©•è«–æ‰“åˆ†ï¼ˆ0~5åˆ†ï¼‰ï¼š\n"
        + all_reviews_content
    )

    print("=== train_prompt å‰ 1000 å­— ===")
    print(train_prompt[:1000])
    print("\n=============================\n")

    # ====== å‘¼å« GPT åšè©•åˆ† ======
    all_scores = []

    for attempt in range(5):
        try:
            print(">> call gpt, reviews:", len(all_reviews_list))
            conversation = client.chat.completions.create(
                # model="gpt-5-mini",
                model="gpt-5-nano",
                messages=[
                    {"role": "system",
                     "content": "ä½ æ˜¯ä¸€ä½å®¢è§€çš„ç¾é£Ÿè©•è«–å®¶ï¼Œåªå›è¦†æ¯å‰‡è©•è«–çš„åˆ†æ•¸ï¼Œæ¯è¡Œä¸€å€‹åˆ†æ•¸ï¼Œä¸è¦å¤šèªªå…¶ä»–æ–‡å­—ã€‚"},
                    {"role": "user", "content": train_prompt}
                ]
            )
            print("<< gpt ok")
            reply = conversation.choices[0].message.content.strip()

            # ä¸€è¡Œä¸€å€‹åˆ†æ•¸
            for line in reply.split("\n"):
                line = line.strip()
                try:
                    score = float(line)
                except:
                    score = ""  # é¿å…éæ•¸å­—é€ æˆå´©æ½°
                all_scores.append(score)

            print("âœ… å®Œæˆ scoring")
            break

        except Exception as e:
            if "Rate limit" in str(e):
                print(f"âš ï¸ rate limitï¼Œç­‰å¾…é‡è©¦... ({attempt+1}/5)")
            else:
                raise e

    # ====== Debugï¼šæª¢æŸ¥å‰ 10 ç­† ======
    print("\n=== å‰ 10 ç­†æª¢æŸ¥ï¼šè©•è«– vs åˆ†æ•¸ ===")
    for i in range(min(10, len(all_reviews_list))):
        print("\n---")
        print(f"è©•è«–ï¼š{all_reviews_list[i]}")
        print(f"GPT çµ¦åˆ†ï¼š{all_scores[i] if i < len(all_scores) else ''}")

    # ====== å°‡ GPT åˆ†æ•¸å¯«å›åŸå§‹ CSV ======
    input_file = output_filename
    output_file = REVIEWS_DIR / "multi_location_reviews_labeled.csv"

    # è®€å–åŸå§‹ CSV
    with open(input_file, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    # åªä¿ç•™é€™äº›æ¬„ä½
    fieldnames = ["shop", "snippet", "date", "rating", "manual"]

    # ç›´æ¥ä¾åºå¯«å…¥ manualï¼ˆä¸ä½¿ç”¨ clean_indicesï¼‰
    for row, score in zip(rows, all_scores):
        row["manual"] = score

    # è‹¥æœ‰å‰©ä¸‹çš„è¡Œæ²’åˆ†æ•¸ï¼Œè£œç©ºå­—ä¸²
    for row in rows:
        if "manual" not in row or row["manual"] == "":
            row["manual"] = ""

    # å¯«å› CSVï¼Œåªä¿ç•™æŒ‡å®šæ¬„ä½
    with open(output_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows([{k: row.get(k, "") for k in fieldnames} for row in rows])

    print(f"âœ… GPT æ¨™è¨»çš„æ–°è©•è«–åˆ†æ•¸å·²å¯«å› {output_file}")

    # console è¼¸å‡ºå‰ 10 ç­†çµæœ
    num_preview = 10
    print(f"\nğŸ”¹ å‰ {num_preview} ç­†æ¨™è¨»çµæœ:")
    for row in rows[:num_preview]:
        print({k: row.get(k, "") for k in fieldnames})

    # ====== ä»¥ä¸‹ Dir(T) æ’åºè¨ˆç®— ======
    input_file = output_file
    df = pd.read_csv(input_file)

    SHOP_COL = "shop"
    DATE_COL = "date"
    MANUAL_COL = "manual"   # æˆ‘å€‘åªç”¨ manual ä½œç‚ºæ˜Ÿç´š

    print("âœ… å·²è®€å…¥ CSVï¼Œå‰ 5 ç­†ï¼š")

    # =========================
    # Step 1. æ—¥æœŸè½‰å¤©æ•¸
    # =========================
    def parse_relative_date(s: str):
        if pd.isna(s):
            return np.nan
        s = str(s).strip()

        if "ï¼š" in s:
            s = s.split("ï¼š", 1)[-1].strip()

        for fmt in ("%Y/%m/%d", "%Y-%m-%d", "%Y.%m.%d"):
            try:
                dt = datetime.strptime(s.split()[0], fmt)
                return (datetime.now() - dt).days
            except Exception:
                pass

        if "ä»Šå¤©" in s:
            return 0
        if "æ˜¨å¤©" in s:
            return 1

        m = re.search(r"(\\d+)\\s*å¹´", s)
        if m:
            return int(m.group(1)) * 365
        m = re.search(r"(\\d+)\\s*å€‹?æœˆ", s)
        if m:
            return int(m.group(1)) * 30
        m = re.search(r"(\\d+)\\s*(é€±|å‘¨|æ˜ŸæœŸ)", s)
        if m:
            return int(m.group(1)) * 7
        m = re.search(r"(\\d+)\\s*å¤©", s)
        if m:
            return int(m.group(1))
        return np.nan

    df["age_days"] = df[DATE_COL].apply(parse_relative_date)
    median_age = df["age_days"].median()
    if pd.isna(median_age):
        median_age = 0
    df["age_days"] = df["age_days"].fillna(median_age)

    # =========================
    # Step 2. è¨ˆç®—æ™‚é–“æ¬Šé‡
    # =========================
    half_life_days = 180.0
    lam = math.log(2) / half_life_days
    df["time_weight"] = np.exp(-lam * df["age_days"])
    df["time_weight"] = df["time_weight"].fillna(1.0)

    # =========================
    # Step 3. å°æ¯é–“åº—åš Dir(T) åˆ†æ•¸ï¼ˆè·³é manual=-1ï¼‰
    # =========================
    MAX_STAR = 5
    PRIOR_ALPHA = 1.0

    def dirT_score_for_shop_manual(group: pd.DataFrame):
        group_valid = group[(group[MANUAL_COL] != -1) & (pd.notna(group[MANUAL_COL]))]

        if group_valid.empty:
            return pd.Series({f"p_{j}": np.nan for j in range(1, MAX_STAR+1)} | {"dirT_score": np.nan})

        ratings = group_valid[MANUAL_COL].astype(float).values
        weights = group_valid["time_weight"].values

        counts = np.zeros(MAX_STAR, dtype=float)
        for r, w in zip(ratings, weights):
            r_int = int(r)
            if 1 <= r_int <= MAX_STAR:
                counts[r_int - 1] += w

        alpha = counts + PRIOR_ALPHA
        probs = alpha / alpha.sum()
        stars = np.arange(1, MAX_STAR + 1, dtype=float)
        score = float(np.dot(stars, probs))

        data = {f"p_{j}": probs[j-1] for j in range(1, MAX_STAR+1)}
        data["dirT_score"] = score
        return pd.Series(data)

    shop_scores = df.groupby(SHOP_COL, sort=False).apply(dirT_score_for_shop_manual).reset_index()
    shop_scores_sorted = shop_scores.sort_values("dirT_score", ascending=False)

    output_scores_file = REVIEWS_DIR / "shop_manual_dirT_scores.csv"
    shop_scores_sorted.to_csv(output_scores_file, index=False)
    print("âœ… å·²è¼¸å‡º CSVï¼š", output_scores_file)

    shop_scores_sorted_records = shop_scores_sorted.to_dict(orient="records")

    return {
        "all_reviews": collected_data,
        "gpt_scores": all_scores,
        "shop_scores_sorted": shop_scores_sorted,
        "shop_scores_sorted_records": shop_scores_sorted_records,
    }


if __name__ == "__main__":
    run_pipeline()
